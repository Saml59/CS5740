{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZNdFlbLJoUXM"},"source":["# Homework 1: Named Entity Recognition (NER) with Sequence Labeling Models\n","## CS4740/5740 Fall 2022\n","\n","### Milestone Submission Due: **September 15, 2022 (11:59 PM)** \n","\n","### Project Submission Due: **September 27, 2022 (11:59 PM)**\n","\n","Should there be major updates to this document, we will announce them on [Ed Stem](https://edstem.org/us/courses/26793/discussion/).  Minor updates are documented on [this particular much-beloved thread](https://edstem.org/us/courses/26793/discussion/1739296).\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"871f2XhgtoYX"},"source":["**Names:**\n","\n","**Netids:**\n","\n","\n","\n","\n","**Editing your version of this notebook:** One partner should make a copy of this notebook and share it with your partner.  **However**, because of synchronization issues (even though Colab works with Google Drive), changes made in this notebook at the same time from different computers/browser windows may not save. We will go so far as to recommend that you close the tab with this notebook when you are not working on it so your partner doesn't face sync issues.\n","\n"]},{"cell_type":"markdown","source":["**Collaboration policy:** please be sure to check the collaboration policy on the [course website](https://courses.cs.cornell.edu/cs4740/2022fa/)!\n","\n"],"metadata":{"id":"QN0nBtOzb5u7"}},{"cell_type":"markdown","source":["> Assignment authors & testers: CS 4740/5740 professors and TAs from this and previous semesters, Chenxin Fang, Meghana Srivastava, Khonzoda Umarova, as well as Ruizhe Wang, Han Xia, and Heather Zhang."],"metadata":{"id":"xY4p6eJmPPY4"}},{"cell_type":"markdown","metadata":{"id":"iguUiw0mor52"},"source":["# **Introduction**\n","---\n","\n","In this project, you will tackle the **Named Entity Recognition** task: you'll implement models that identify named entities in text and tag them with the appropriate label. A primer on this task is provided further on.  We will treat this as a **sequence-tagging task**: for each token in the input text, assign one of the following 5 entity labels -- **ORG** (Organization), **PER** (Person), **LOC** (Location), **MISC** (Miscellaneous), and **O** (Not Named Entity) -- as well as a BIO-format prefix **B-** (token is the *beginning* of a named entity) or **I-** (token is *inside* a named entity). Overall, this yields 9 different labels: **B-ORG, I-ORG, B-PER, I-PER, B-LOC, I-LOC, B-MISC, I-MISC** and **O**.\n","\n","For this project, you will implement two sequence labeling approaches:\n","- Model 1 : a Hidden Markov Model (HMM)\n","- Model 2 : a Maximum Entropy Markov Model (MEMM)/Logistic Regression classifier (also known as a MaxEnt classifier). Feature engineering is strongly suggested for this model!\n","\n","A key component of both models is implementation of the Viterbi algorithm, which we will use to find the most likely tag sequence to assign to an input text."]},{"cell_type":"markdown","metadata":{"id":"mib4pTXj3hir"},"source":["## **Logistics**\n","\n","---\n","\n","- You are **strongly encouraged** to work in **groups of 2 students**. Students in the same group will get the same grade. Thus, you should make sure that everyone in your group contributes to the project. \n","- **Do not form teams of two on Kaggle** (You *will* form teams on a different platform; details TBA) because submitting separately gives you more tries, which is useful when experimenting with different models. \n","- **Before submitting your predictions on Kaggle**, please rename your \"Kaggle team\" (since you and your partner are not forming your group on the Kaggle platform, this \"team\" would actually only include you) to be your NetID (eg: `team_ku47`). You can do so under `Team` tab of the competition. \n","- A part of your submission would involve uploading your notebook (details would be provided soon!). So, please enter all code and answer all the questions in this colab notebook.\n","  - Your code must have docstrings/comments documenting the meaning of parameters and important parameter-like variables.\n","- In this assignment you are asked to make two submissions:\n","  1. Intermediate **milestone submission due on 9/15/22 (11:59 PM)**. For this, please 1) have your teams formed (mechanism TBA) and  2) submit predictions of your first model (HMM) on Kaggle. This means that you should aim to complete Part 1 and Part 2 of the assignment by this milestone. Points will be awarded for meeting the milestone deadline, but we will only grade for completion, not correctness. \n","  2. The **final homework submission due on 9/27/22 (11:59 PM)**. (details TBA)\n","- Please be sure to consult the [list](https://docs.google.com/document/d/1-QmpkZYJDCM4gQQ9sZW2EwD5ltYy1CFjlJTbhD0Be-A/edit?usp=sharing) of banned packages/libraries before you start implementing your models. Note that this list may get updated.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"q2z7TIHV3kCH"},"source":["## **Advice**\n","\n","---\n","\n","1. Please read through the entire notebook before you start coding. That might inform your code structure.\n","2. An assignment outline and grading breakdown (subject to minor adjustments) is found below; please consult it.\n","3. The project is somewhat open ended. We will ask you to implement the models, but in some cases precise data structures and so on can be chosen by you. However, to integrate with Kaggle, you will need to submit Kaggle predictions using the given evaluation code (more instructions later)."]},{"cell_type":"markdown","source":["<a name=\"outline\"></a>\n","## **Assignment outline and grading breakdown**\n","- [Part 1](#part1)\n","  - [Q1](#q1) [10 pts]\n","- [Part 2](#part2)\n","  - [Unknown Word Handling](#unknowns_handling) [15 pts]\n","  - [HMM Implementation](#hmm_implementation) [20 pts]\n","  - [Viterbi Implementation](#viterbi_implementation) [20 pts]\n","  - [Validation](#validation_data) [3 pts]\n","  - [Q2.1](#q2.1) [5 pts]\n","  - [Q2.2](#q2.2) [5 pts]\n","  - [Q2.3](#q2.3) [5 pts]\n","  - [Q2.4](#q2.4) [5 pts]\n","- [Part 3](#part3)\n","  - [Features](#features) [35 pts]\n","  - [MEMM Implementation](#memm_implementation) [25 pts]\n","  - [Q3.1](#q3.1) [5 pts]\n","  - [Q3.2](#q3.2) [5 pts]\n","  - [Q3.3](#q3.3) [5 pts]\n","  - [Q3.4](#q3.4) [5 pts]\n","- [Part 4](#part4)\n","  - [Q4.1](#q4.1) [7 pts]\n","  - [Q4.2](#q4.2) [7 pts]\n","  - [Q4.3](#q4.3) [7 pts]\n","- [Part 5](#part5)\n","  - [Q5](#q5)\n","\n","\n","Meeting the milestone deadline [10 pts];\n","\n","Outperforming our baseline on Kaggle [15 pts];\n","\n"],"metadata":{"id":"NyokCzP2Zqqx"}},{"cell_type":"markdown","metadata":{"id":"kBsjyfPu5V4t"},"source":["## **Named Entity Recognition: Review**\n","\n","---\n","\n","NER refers to the information extraction technique of identifying and categorizing key information about entities within textual data. NER is important for:\n","  - Detecting entities in search engines and voice assistants for more relavent search results.\n","  - Automatically parsing resumes.\n","  - ...and much more!\n","\n","In our dataset named entity tags are formatted in BIO/IOB format. With this format, entity tags get a prefix. Prefix \"B-\" is added to the first word/token of the entity name. All following tokens that are part of the same entity name would get prefix \"I-\". \n","\n","Here is an example sentence: \"ZIFA\n","said\n","Renate\n","Geotschel\n","of\n","Austria\n","won the women's World Cup  downhill race in Germany.\"\n","Entity \"Renate Goetschl\" gets \"Renate\" (B-PER) and \"Goestchl\" (I-PER). Similarly, for \"World Cup\" we'd have \"World\" (B-MISC) and \"Cup\" (I-MISC). If an entity only has one token, then its entity tag would still have prefix \"B-\". \"O\" is used to denote tokens that are not part of any named entity. Thus, from the example above, we'd have:\n","\n","```\"ZIFA\" B-ORG```\n","\n"," ```\"said\" O```\n","\n"," ```\"Renate\" B-PER```\n","\n"," ```\"Goetschl\" I-PER```\n","\n"," ```\"of\" O```\n","\n"," ```\"Austria\" B-LOC```\n","\n"," ```\"won\" O```\n","\n"," ```\"the\" O```\n","\n"," ```\"women's\" O```\n","\n"," ```\"World\" B-MISC```\n","\n"," ```\"Cup\" I-MISC```\n","\n"," ```\"downhill\" O```\n","\n"," ```\"race\" O```\n","\n"," ```\"in\" O```\n","\n"," ```\"Germany\" B-LOC```\n","\n","\n","Although NER is predominantly handled by deep learning approaches, for now let's use HMMs and MEMMs. \n","\n","\n","To read more on NER, we refer to any of the following sources:\n","1. Medium post [1](https://umagunturi789.medium.com/everything-you-need-to-know-about-named-entity-recognition-2a136f38c08f) and [2](https://medium.com/mysuperai/what-is-named-entity-recognition-ner-and-how-can-i-use-it-2b68cf6f545d).\n","2. Try out [this](https://demo.allennlp.org/named-entity-recognition/named-entity-recognition) AlllenNLP demo! Please note that this demo uses a slightly different format of NER tags."]},{"cell_type":"markdown","metadata":{"id":"DSFfegs8LKY8"},"source":["## **Evaluation: Entity Level Mean F1**\n","\n","---\n","\n","The standard evaluation measures to report for NER are recall, precision, and F1 score\n","(also called F-measure) evaluated at the **named entity level** (not at the token level). The code for this has been provided later under the validation section under Part 2. Please use this code when evaluating your models. \n","\n","\n","If P and T are the sets of predicted and true *named entity spans*, respectively, (e.g, the five named entity spans in the above example are \"Zifa\", \"Renate Goetschl\", \"Austria\", \"World Cup\", and \"Germany\") then\n","\n","####<center>Precision = $\\frac{|\\text{P}\\;\\cap\\;\\text{T}|}{|\\text{P}|}$ and Recall = $\\frac{|\\text{P}\\;\\cap\\;\\text{T}|}{|\\text{T}|}$.</center><br/>\n","\n","\n","####<center>F1 = $\\frac{2 * \\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}$. </center><br/>\n","\n","For each type of named entity, e.g. *LOC*ation, *MISC*ellaneous, *ORG*anization and *PER*son, we calculate the F1 score as shown above, and take the mean of all these F1 scores to get the **Entity Level Mean F1** score for the test set. If $N$ is the total number of labels (i.e., named entity types), then\n","\n","####<center>Entity Level Mean F1 = $\\frac{\\sum_{i = 1}^{N} \\text{F1}_{{label}_i}}{N}$. </center>\n","\n","More details under the validation section in Part 2.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7iP63fHj5saG"},"source":["<a name=\"part1\"></a>\n","# **Part 1: Dataset**\n","[[^^^]](#outline) \n","\n","Our data is a modified version of the WikiNEuRal ([ Tedeschi et al.](https://aclanthology.org/2021.findings-emnlp.215.pdf)) dataset."]},{"cell_type":"markdown","metadata":{"id":"pljkH2ow5U9x"},"source":["Load the dataset as follows:\n","  1. Obtain the data from Data tab of the [Kaggle competition](https://www.kaggle.com/t/200697e4726f448986930dd4e823e957).\n","  2. Unzip the data. Put it into your Google Drive, run the cells below to mount it to Colab:\n"]},{"cell_type":"code","metadata":{"id":"c3dQChuccqfN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7cdc2b5-f721-4e15-e53c-b0ecc3070b8c","executionInfo":{"status":"ok","timestamp":1662925895466,"user_tz":240,"elapsed":17901,"user":{"displayName":"Samuel Lee","userId":"00207309823749368828"}}},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"uFXI7NRHn1Cc"},"source":["import json\n","\n","# TODO: please change the line below with your drive organization\n","path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"NLP\", \"HW1\", \"data\")\n","\n","with open(os.path.join(path,'train.json'), 'r') as f:\n","     train = json.loads(f.read())\n","\n","with open(os.path.join(path,'val.json'), 'r') as f:\n","     val = json.loads(f.read())\n","\n","with open(os.path.join(path,'test.json'), 'r') as f:\n","     test = json.loads(f.read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zfjKFeE_7T7C"},"source":["Here's a few things to note about the dataset above:\n","1. We have loaded 3 `.json` files: for training, validation, and testing.\n","2. The train and validation files contain the following 3 fields (each is a nested list): \n","  - **'text'** - actual input tokens\n","  - **'NER'** - the token-level entity tag \n","  - **'index'** - index of the token in the dataset\n","3. The test data only has **'text'**, and **'index'** fields. You will need to submit your prediction of the **'NER'** tag to Kaggle."]},{"cell_type":"markdown","metadata":{"id":"cradDk-37G8L"},"source":["\n","### **Q1: Initial Data Observations**\n","\n","In the space below please add your code for dataset explorations for Q1."]},{"cell_type":"code","metadata":{"id":"PvvRSlhb6sAR","colab":{"base_uri":"https://localhost:8080/","height":661},"executionInfo":{"status":"error","timestamp":1662927236445,"user_tz":240,"elapsed":414,"user":{"displayName":"Samuel Lee","userId":"00207309823749368828"}},"outputId":"a3eac7d8-1acc-46e2-dfa8-15aec6d8cccd"},"source":["## add your code here\n","import pandas as pd\n","#print(test['text'],len(test['text']), test['index'], len(test['index']))\n","#print(train['text'][0])\n","hist = {}\n","for seq in train['NER'] :\n","  removedzeroes = [tag for tag in seq if len(tag) > 1]\n","  hist[len(removedzeroes)] = hist.get(len(removedzeroes), 0) + 1\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{1: 2543, 3: 974, 4: 653, 2: 1761, 7: 128, 5: 378, 9: 64, 6: 235, 22: 2, 10: 50, 8: 89, 11: 36, 12: 26, 14: 15, 15: 10, 17: 5, 16: 4, 13: 15, 18: 6, 19: 2, 21: 1, 25: 1, 27: 1, 23: 1}\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-df5023914f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m n, bins, patches = plt.hist(x=hist, bins='auto', color='#0504aa',\n\u001b[0;32m---> 11\u001b[0;31m                             alpha=0.7, rwidth=0.85)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of tags'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, data, **kwargs)\u001b[0m\n\u001b[1;32m   2608\u001b[0m         \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m         color=color, label=label, stacked=stacked, **({\"data\": data}\n\u001b[0;32m-> 2610\u001b[0;31m         if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[1;32m   6628\u001b[0m                     \u001b[0;31m# python's min/max ignore nan,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6629\u001b[0m                     \u001b[0;31m# np.minnan returns nan for all nan input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6630\u001b[0;31m                     \u001b[0mxmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6631\u001b[0m                     \u001b[0mxmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6632\u001b[0m             \u001b[0;31m# make sure we have seen at least one non-nan and finite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'dict' and 'float'"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["import numpy as np\n","\n","\n"],"metadata":{"id":"pl6_ZBtjMZBi","executionInfo":{"status":"ok","timestamp":1662927362069,"user_tz":240,"elapsed":8,"user":{"displayName":"Obinna Abii","userId":"08278722797785707691"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sO7go9ivDusU"},"source":["<a name=\"q1\"></a>\n","[[^^^]](#outline) \n","\n","What are your initial observations after you explore the dataset?  Provide some quantitative data exploration. Assess dataset size, document lengths and the token-level NER class distribution, and the entity-level NER class distribution (skipping the 'O' label for the latter). Give some examples of sentences with their named entities bracketed, e.g. [[B-LOC Romania] state budget soars in June .] and [[B-ORG Zifa] said [B-PER Renate] [I-PER Goetschl] of [B-LOC Austria]...]. \n"]},{"cell_type":"markdown","source":["#### **A1:**\n","\n","... add your answers here"],"metadata":{"id":"dVbqyFYq1tOy"}},{"cell_type":"markdown","metadata":{"id":"4NYvfchqqBf6"},"source":["<a name=\"part2\"></a>\n","# **Part 2: Hidden Markov Model**\n","[[^^^]](#outline) \n","---\n","In this part of the assignment, you will:\n","1. Implement code for counting and smoothing of labels and words, as well as unkown word handing, as necessary to support the Viterbi algorithm. \n","2. Build a Hidden Markov Model in accordance with the provided function headers. **You may NOT change the function specifications.** Please ensure that your code is clear, concise, and, most important of all, modular. This means you should break your implementation down into smaller functions or write it within a class. Please compute all probabilities in natural log when building the HMM.\n","3. Implement the **Viterbi algorithm**, that can be used to infer token-level labels (identifying the appropriate named entity) for an input document. This process is commonly referred to as **decoding**. Bigram-based Viterbi is $ \\mathcal{O}(sm^2)$ where *s* is the length of the sentence and *m* is the number of tags. Your implementation should have similar efficiency. The code for this can be used later on for the MEMM too.\n","\n","### References\n","You may find chapters [3](https://web.stanford.edu/~jurafsky/slp3/3.pdf) and [8](https://web.stanford.edu/~jurafsky/slp3/8.pdf) of Jurafsky and Martin book useful. In particular, section 3.4.1 covers ways to handle unknown words, and section 3.5 goes over smoothing. "]},{"cell_type":"markdown","metadata":{"id":"-jUVJwSaE1tI"},"source":["<a name=\"unknowns_handling\"></a>\n","## **Unknown Word Handling**\n","[[^^^]](#outline) \n","---"]},{"cell_type":"code","metadata":{"id":"44Bja4eQEMJR"},"source":["# Implement unknown word handling here! You may do this any way that you please\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aLuTFUV5FA3m"},"source":["<a name=\"hmm_implementation\"></a>\n","## **HMM Implementation**\n","[[^^^]](#outline) \n","---\n","\n","In the skeleton code below, we have broken down the HMM into its three components: the transition matrix, the emission matrix, and the start state probabilities. We suggest you implement them separately and then use them to build the HMM.\n"]},{"cell_type":"code","metadata":{"id":"NBJgSrRtFZuG"},"source":["# Returns the transition probabilities.\n","def build_transition_matrix(labels, k=0):\n","  \"\"\"\n","    Returns a dictionary that has tuples of every label bigram as keys, and\n","    the associated value being the respective transition probabilities (in \n","    natural log).\n","\n","    Eg. {(\"O\", \"B-ORG\"): -9.98690147425591, \n","         (\"B-LOC\", \"I-LOC\"): -3.69537214,\n","         ...,\n","         ...,\n","        }\n","    \n","    :parameter labels: A list where each element represents a sentence, \n","    and each sentence is a list of NER labels for each of its tokens. (Eg. \n","    [['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], [...], ...])\n","    :type labels: List[List[String]]\n","    :parameter k: an optional parameter for smoothing\n","    :type k: int\n","    \"\"\"\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gRt-pjh4FvZ2"},"source":["# Returns the emission probabilities.\n","def build_emission_matrix(tokens, labels, k=0):\n","  \"\"\"\n","    Returns a dictionary that has label-token tuples as keys, and emission \n","    probabilities (in natural log) for each respective label-token pair as values.  \n","\n","    Eg. {(\"O\", \"Because\"): -10.133904545421267, \n","         (\"I-PER\", \"Markov\"): -7.428569227340841,\n","         ...,\n","         ...,\n","        }\n","    \n","    :parameter tokens: A list where each element represents a sentence, \n","    and each sentence is a list of its tokens. (Eg. [['The', 'most', \n","    'significant', 'damage', 'was', 'on', 'Tortola', '.'], [...], ...])\n","    :type tokens: List[List[String]]\n","    :parameter labels: A list where each element represents a sentence, \n","    and each sentence is a list of NER labels for each of its tokens. (Eg. \n","    [['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], [...], ...])\n","    :type labels: List[List[String]]\n","    :parameter k: an optional parameter for smoothing\n","    :type k: int\n","    \"\"\"\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTN9B6k0HK77"},"source":["# Returns the starting state probabilities.\n","def get_start_state_probs(labels, k=0):\n","  \"\"\"\n","    Returns a dictionary that has labels for keys, and the respective state \n","    probabilities (in natural log) for values.  \n","\n","    Eg. {\"O\": -10.133904545421267, \n","         \"I-PER\": -7.428569227340841,\n","         ...,\n","         ...,\n","        }\n","    \n","    :parameter labels: A list where each element represents a sentence, \n","    and each sentence is a list of NER labels for each of its tokens. (Eg. \n","    [['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], [...], ...])\n","    :type labels: List[List[String]]\n","    :parameter k: an optional parameter for smoothing\n","    :type k: int\n","    \"\"\"\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LU3Ff-CKInqo"},"source":["# Takes in the tokens & labels and returns a representation of the HMM.\n","# Call the three functions above in this function to build your HMM.\n","def build_hmm(tokens, labels):\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pbw3RTHPI31j"},"source":["<a name=\"viterbi_implementation\"></a>\n","## **Viterbi Implementation**\n","[[^^^]](#outline) \n","---\n","\n","At the end of your implementation, we expect a function or class that maps a sequence of tokens (observation) to a sequence of labels via the Viterbi algorithm."]},{"cell_type":"code","metadata":{"id":"C_q3U42lI3LQ"},"source":["# Takes in the HMM built above and an observation (i.e. a list of tokens),\n","# and returns a list with predicted named entity mappings for the tokens.\n","# The returned list should be the same length as the input obeservation.\n","def viterbi(hmm, observation):\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ie6NXgJAKOEM"},"source":["# Here's a sample observation that you can use to test your code\n","obs_1 = ['Cornelll',\n"," 'University',\n"," 'is',\n"," 'located',\n"," 'in',\n"," 'Ithaca',\n"," 'and',\n"," 'was',\n"," 'founded',\n"," 'by',\n"," 'Ezra',\n"," 'Cornell']\n","\n","# Uncomment and fill out the following line to test your implementation:\n","# viterbi(obs=obs_1, hmm=?)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2J8NdM_V_A4u"},"source":["## **Validation Step**\n","<a name=\"validation_data\"></a>\n","[[^^^]](#outline) \n","---\n","\n","In this part of the project, we expect you to train your HMM model (i.e., get transition and emission probabilities) on the labeled training data and evaluate it on the validation data. Report **Entity Level Mean F1**, which was explained earlier. Please use the code we have provided below to compute this metric.\n","\n","Please also take a look into your misclassified cases, as we will be performing error analysis in the *Evaluation* section. We expect smoothing, unknown word handling and correct emission (i.e., lexical generation) probabilities."]},{"cell_type":"markdown","metadata":{"id":"oTAhu_TG1V0R"},"source":["Consider the example below. After getting a sequence of NER labels for the sequence of tokens from your Viterbi algorithm implementation, you need to convert the sequence of tokens, associated token indices and NER labels into a format which can be used to calculate **Entity Level Mean F1**. We do this by finding the starting and ending indices of the spans representing each entity (as given in the corpus) and adding it to a list that is associated with the label with which the spans are labelled. To score your validation data on Google Colab or your local device, you can get a dictionary from the function **format_output_labels** on both the predicted and true label sequences, and use the two dictionaries as input to the **mean_f1** function.\n","\n","NOTE: We do **not** include the spans of the tokens labelled as \"O\" in the formatted dictionary output."]},{"cell_type":"code","metadata":{"id":"HdOOQdN7D2rv"},"source":["def format_output_labels(token_labels, token_indices):\n","    \"\"\"\n","    Returns a dictionary that has the labels (LOC, ORG, MISC or PER) as the keys, \n","    with the associated value being the list of entities predicted to be of that key label. \n","    Each entity is specified by its starting and ending position indicated in [token_indices].\n","\n","    Eg. if [token_labels] = [\"B-ORG\", \"I-ORG\", \"O\", \"O\", \"B-ORG\"]\n","           [token_indices] = [15, 16, 17, 18, 19]\n","        then dictionary returned is \n","        {'LOC': [], 'MISC': [], 'ORG': [(15, 16), (19, 19)], 'PER': []}\n","\n","    :parameter token_labels: A list of token labels (eg. B-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG, B-MISC, OR I-MISC).\n","    :type token_labels: List[String]\n","    :parameter token_indices: A list of token indices (taken from the dataset) \n","                              corresponding to the labels in [token_labels].\n","    :type token_indices: List[int]\n","    \"\"\"\n","    label_dict = {\"LOC\":[], \"MISC\":[], \"ORG\":[], \"PER\":[]}\n","    prev_label = 'O'\n","    start = token_indices[0]\n","    for idx, label in enumerate(token_labels):\n","      curr_label = label.split('-')[-1]\n","      if label.startswith(\"B-\") or (curr_label != prev_label and curr_label != \"O\"):\n","        if prev_label != \"O\":\n","            label_dict[prev_label].append((start, token_indices[idx-1]))\n","        start = token_indices[idx]\n","      elif label == \"O\" and prev_label != \"O\":\n","        label_dict[prev_label].append((start, token_indices[idx-1]))\n","        start = None\n","\n","      prev_label = curr_label\n","    if start is not None:\n","      label_dict[prev_label].append((start, token_indices[idx-1]))\n","    return label_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfjVJLNhL_fc"},"source":["# Code for mean F1\n","\n","import numpy as np\n","\n","def mean_f1(y_pred_dict, y_true_dict):\n","    \"\"\" \n","    Calculates the entity-level mean F1 score given the actual/true and \n","    predicted span labels.\n","    :parameter y_pred_dict: A dictionary containing predicted labels as keys and the \n","                            list of associated span labels as the corresponding\n","                            values.\n","    :type y_pred_dict: Dict<key [String] : value List[Tuple]>\n","    :parameter y_true_dict: A dictionary containing true labels as keys and the \n","                            list of associated span labels as the corresponding\n","                            values.\n","    :type y_true_dict: Dict<key [String] : value List[Tuple]>\n","\n","    Implementation modified from original by author @shonenkov at\n","    https://www.kaggle.com/shonenkov/competition-metrics.\n","    \"\"\"\n","    F1_lst = []\n","    for key in y_true_dict:\n","        TP, FN, FP = 0, 0, 0\n","        num_correct, num_true = 0, 0\n","        preds = y_pred_dict[key]\n","        trues = y_true_dict[key]\n","        for true in trues:\n","            num_true += 1\n","            if true in preds:\n","                num_correct += 1\n","            else:\n","                continue\n","        num_pred = len(preds)\n","        if num_true != 0:\n","            if num_pred != 0 and num_correct != 0:\n","                R = num_correct / num_true\n","                P = num_correct / num_pred\n","                F1 = 2*P*R / (P + R)\n","            else:\n","                F1 = 0      # either no predictions or no correct predictions\n","        else:\n","            continue\n","        F1_lst.append(F1)\n","    return np.mean(F1_lst)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rwLFPhetVok","outputId":"67280996-54af-42ea-dad3-28cc61dd55c6"},"source":["# Usage using above example\n","\n","pred_token_labels = [\"B-ORG\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"B-MISC\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\"]\n","true_token_labels = [\"B-ORG\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"O\", \"O\", \"O\", \"B-LOC\"]\n","token_indices = [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n","\n","y_pred_dict = format_output_labels(pred_token_labels, token_indices)\n","print(\"y_pred_dict is : \" + str(y_pred_dict))\n","y_true_dict = format_output_labels(true_token_labels, token_indices)\n","print(\"y_true_dict is : \" + str(y_true_dict))\n","\n","print(\"Entity Level Mean F1 score is : \" + str(mean_f1(y_pred_dict, y_true_dict)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["y_pred_dict is : {'LOC': [(18, 18), (28, 27)], 'MISC': [(23, 23)], 'ORG': [(13, 13)], 'PER': [(15, 16)]}\n","y_true_dict is : {'LOC': [(18, 18), (28, 27)], 'MISC': [(23, 24)], 'ORG': [(13, 13)], 'PER': [(15, 16)]}\n","Entity Level Mean F1 score is : 0.75\n"]}]},{"cell_type":"code","metadata":{"id":"0Lmx9z5WGAfq"},"source":["# Evaluate/validate your model here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YRLi5TDyuSx_"},"source":["<a name=\"q2.1\"></a>\n","[[^^^]](#outline) \n","## **Q2.1: Explain your HMM Implementations**\n","\n","Explain how you implemented the HMM including the Viterbi algorithm. Make clear which parts were implemented from scratch vs. obtained via an existing package (review the Logistics section for information on packages that are not allowed). Explain and motivate any design choices providing the intuition behind them.\n"]},{"cell_type":"markdown","source":["#### **A2.1:**\n","\n","... add your answers here"],"metadata":{"id":"sL2bUPVkNpbt"}},{"cell_type":"markdown","metadata":{"id":"DHbzRuil-yjG"},"source":["<a name=\"q2.2\"></a>\n","[[^^^]](#outline) \n","## **Q2.2: Results Analysis**\n","\n","Explain here how you evaluated the models. Summarize the performance of your system and any variations that you experimented with on the validation datasets. Put the results into clearly labeled tables or diagrams and include your observations and analysis.\n"]},{"cell_type":"markdown","source":["#### **A2.2:**\n","\n","... add your answers here"],"metadata":{"id":"j_aBZqFKOKRO"}},{"cell_type":"markdown","metadata":{"id":"rTIPGnLFNc43"},"source":["<a name=\"q2.3\"></a>\n","[[^^^]](#outline) \n","## **Q2.3: Error Analysis**\n","When did the system work well? When did it fail?  Any ideas as to why? How might you improve the system?\n"]},{"cell_type":"markdown","source":["#### **A2.3:**\n","\n","... add your answers here"],"metadata":{"id":"u7j0YVozOMIT"}},{"cell_type":"markdown","metadata":{"id":"mf6ziT36NteS"},"source":["<a name=\"q2.4\"></a>\n","[[^^^]](#outline) \n","## **Q2.4: What is the effect of unknown word handling and smoothing?**"]},{"cell_type":"markdown","source":["#### **A2.4:**\n","\n","... add your answers here"],"metadata":{"id":"q9PL6823OQPT"}},{"cell_type":"markdown","metadata":{"id":"31e3sMHZrLWP"},"source":["<a name=\"part3\"></a>\n","# **Part 3: Maximum Entropy Markov Model** \n","[[^^^]](#outline) \n","---\n","\n","In this section, you will implement a Maximum Entropy Markov Model (**MEMM**) to perform the same NER task. Your model should consist of a MaxEnt classifier with Viterbi decoding. \n","\n","1. We have already performed tokenizations for documents. You can either use a MaxEnt classifier from an existing package or write the MaxEnt code yourself. **Important note:  MaxEnt classifiers are statistically equivalent to multi-class logistic regression, so you can use packages for multi-class LR instead of MaxEnt.**\n","\n","2. Use the classifier to learn a probability $P(t_i|features)$. You may replace either the lexical generation probability – $P(w_i|t_i)$ – or the transition probability – $P(t_i|t_{i−1})$ – in the HMM with it, or you may replace the entire *lexical generation probability * transition probability*  calculation – $P (w_i|t_i) ∗ P (t_i|t_{i−1)} – $ in the HMM with it. \n","\n","3. To train such a classifier, you need to pick some feature set. The content of the feature set is up to you, but try different ones, and evaluate your choices on the validation set. Pick the feature set that performs overall the best according to the F1 measure. If you draw inspiration for your features from external sources, please link them in the code.\n","  * While there are many directions to take when looking for features, you may start by exploring parts of speech that appear in sentences. There are several libraries (ex. [nltk](https://www.nltk.org/book/ch05.html)) that process sentences and identify parts of speech. If you end up using a library to extract parts of speech tags or other features, please indicate this in your asnwer to Q3.1.\n","\n","4. Use your own implementation of the **Viterbi algorithm**, which you can modify from the one you developed for the HMM model. You will need the probabilities that you obtain from the MaxEnt classifier. \n","\n","5. Remember to use same training and validation split when evaluating the MEMM to have a **fair comparison** with your **HMM model**.\n","\n","\n","Please also take a look into your misclassified cases, as we will be performing error analysis in Part 4. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"PJIosHVJZ-1o"},"source":["\n","\n","---\n","Here's a summary of the workflow for Part 3:\n","\n","![alt text](https://drive.google.com/uc?export=view&id=14VfjW3yDyXLojWM_u0LeJYdDOSLkElBn)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8QGSijPUW_Bi"},"source":["Note that we have not provided any skeleton code for how you should do feature engineering since this is meant to be an open ended task and we want you to experiment with the dataset. However, please remember to make sure that you code is concise, clean, and readable! Ultimately, we expect a function or class  mapping a sequence of tokens to a sequence of labels. At the end of this section you should have done the following:\n","1. Extract Features\n","2. Build & Train MaxEnt\n","3. Call Viterbi when evaluating\n","\n","### References\n","You may find [chapter 8](https://web.stanford.edu/~jurafsky/slp3/8.pdf) of Jurafsky and Martin book useful. In particular, you could consider section 8.5.2 for features in NER. "]},{"cell_type":"markdown","metadata":{"id":"Pd2PwG4wYkhQ"},"source":["<a name=\"features\"></a>\n","## **Feature Engineering**\n","[[^^^]](#outline) \n","---"]},{"cell_type":"code","metadata":{"id":"6xBYPGLUHH7n"},"source":["# Your implementation here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jCkP4A3IYwit"},"source":["<a name=\"memm_implementation\"></a>\n","## **MEMM Implementation**\n","[[^^^]](#outline) \n","---"]},{"cell_type":"code","metadata":{"id":"iAR5IjDxY4GJ"},"source":["# Your implementation here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vvoeAVMlX4gp"},"source":["### **Validation**\n","---\n","In this section we want you to run your MaxEnt model on the validation dataset you extracted earlier. We want you to play around with different combinations of features in order to find which features work the best for your implementation. You will be asked to write about this process in detail in written question 3.3 so please spend time experimenting with features! Once again, please use the code we provided for computing Entity Level Avg F1 earlier when validating your model."]},{"cell_type":"code","metadata":{"id":"TmUnuuxvHKXM"},"source":["# Run your model on validation set\n","# You will need to \n","# 1. Call your function above to get a prediction result on Validation Set\n","# 2. Report Metrics\n","# (See if you need to modify your feature set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fGkhL1imxpmH"},"source":["<a name=\"q3.1\"></a>\n","[[^^^]](#outline) \n","\n","## **Q3.1: Implementation Details**\n","Explain how you implemented the MEMM and whether/how you modified Viterbi (e.g. which algorithms/data structures you used, what features are included). Make clear which parts were implemented from scratch vs. obtained via an existing package.\n"]},{"cell_type":"markdown","source":["#### **A3.1:**\n","\n","... add your answers here"],"metadata":{"id":"etkqBEJxOZbI"}},{"cell_type":"markdown","metadata":{"id":"m_eDwiILvHGL"},"source":["<a name=\"q3.2\"></a>\n","[[^^^]](#outline) \n","\n","## **Q3.2: Results Analysis**\n","Explain here how you evaluated the MEMM model. Summarize the performance of your system and any variations that you experimented with the validation datasets. Put the results into clearly labeled tables or diagrams and include your observations and analysis.\n"]},{"cell_type":"markdown","source":["#### **A3.2:**\n","\n","... add your answers here"],"metadata":{"id":"udTvSjp1ObVy"}},{"cell_type":"markdown","metadata":{"id":"ammZn20RZn8h"},"source":["<a name=\"q3.3\"></a>\n","[[^^^]](#outline) \n","\n","## **Q3.3: Feature Engineering**\n","What features are considered most important by your MaxEnt Classifier? Why do you think these features make sense? Describe your experiments with feature sets. An analysis on feature selection for the MEMM is required – e.g. what features **help most**, why? An **error analysis** is required – e.g. what sorts of errors occurred, why?"]},{"cell_type":"markdown","source":["#### **A3.3:**\n","\n","... add your answers here"],"metadata":{"id":"foadgkcOOcvT"}},{"cell_type":"markdown","metadata":{"id":"h4XaDMlcaDBA"},"source":["<a name=\"q3.4\"></a>\n","[[^^^]](#outline) \n","\n","## **Q3.4: Room for Improvement**\n","When did the system work well, when did it fail and any ideas as to why? How might you improve the system? \n"]},{"cell_type":"markdown","source":["#### **A3.4:**\n","\n","... add your answers here"],"metadata":{"id":"Q_esMp_pOeGD"}},{"cell_type":"markdown","metadata":{"id":"lQwPwqU3vMiS"},"source":["<a name=\"part4\"></a>\n","# **Part 4: Comparing HMMs and MEMMs**\n","[[^^^]](#outline) \n","\n","---\n","\n","In this section you will be asked to analyze and compare the models you have developed!\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ndVxVFzFagZ5"},"source":["<a name=\"q4.1\"></a>\n","[[^^^]](#outline) \n","\n","## **Q4.1: Result Comparison**\n","Compare here your results (validation scores) for your HMM and the MEMM. Which of them performs better? Why?"]},{"cell_type":"markdown","source":["#### **A4.1:**\n","\n","... add your answers here"],"metadata":{"id":"FuXjldBAOgXf"}},{"cell_type":"markdown","metadata":{"id":"cjGcdm5aafl3"},"source":["<a name=\"q4.2\"></a>\n","[[^^^]](#outline) \n","\n","## **Q4.2: Error Analysis 1**\n","Do some error analysis. What are error patterns you observed that the HMM makes but the MEMM does not? Try to justify why/why not? **Please give examples from the dataset.**"]},{"cell_type":"markdown","source":["#### **A4.2:**\n","\n","... add your answers here"],"metadata":{"id":"NzFPgH9MOiQw"}},{"cell_type":"markdown","metadata":{"id":"rZYfZAgga9UB"},"source":["<a name=\"q4.3\"></a>\n","[[^^^]](#outline) \n","\n","## **Q4.3: Error Analysis 2**\n","What are error patterns you observed that MEMM makes but the HMM does not? Try to justify what you observe? **Please give examples from the dataset.** "]},{"cell_type":"markdown","source":["#### **A4.3:**\n","\n","... add your answers here"],"metadata":{"id":"Shm8ZaaoOjbg"}},{"cell_type":"markdown","metadata":{"id":"TpugxBD7RBy1"},"source":["<a name=\"part5\"></a>\n","# **Part 5: Kaggle Submission**\n","[[^^^]](#outline) \n","\n","---\n","\n","Using the best-performing system from among all of your HMM and MEMM models, generate predictions for the test set, and submit them to [Kaggle competition](https://www.kaggle.com/t/200697e4726f448986930dd4e823e957). Below, we provide a function that submits given predicted tokens and associated token indices in the correct format. As a scoring metric on Kaggle, we use **Entity Level Mean F1**.\n","\n","Your submission to Kaggle should be a CSV file consisting of five lines and two columns. The first line is a fixed header, and each of the remaining four lines corresponds to one of the four types of named entities. The first column is the label identifier *Id* (one of PER, LOC, ORG or MISC), and the second column *Predicted* is a list of entities (separated by single space) that you predict to be of that type. Each entity is specified by its starting and ending index (concatenated by a hypen) as given in the test corpus. \n","\n","You can use the function **create_submission** that takes the list of predicted labels and the list of associated token indices as inputs and creates the the output CSV file at a specified path.\n","\n","NOTE: Ensure that there are **no** rows with *Id* = \"O\" in your Kaggle Submission."]},{"cell_type":"code","metadata":{"id":"893l9j77ETFM"},"source":["import csv\n","\n","def create_submission(output_filepath, token_labels, token_inds):\n","    \"\"\"\n","    :parameter output_filepath: The full path (including file name) of the output file, \n","                                with extension .csv\n","    :type output_filepath: [String]\n","    :parameter token_labels: A list of token labels (eg. PER, LOC, ORG or MISC).\n","    :type token_labels: List[String]\n","    :parameter token_indices: A list of token indices (taken from the dataset) \n","                              corresponding to the labels in [token_labels].\n","    :type token_indices: List[int]\n","    \"\"\"\n","    label_dict = format_output_labels(token_labels, token_inds)\n","    with open(output_filepath, mode='w') as csv_file:\n","        fieldnames = ['Id', 'Predicted']\n","        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","        writer.writeheader()\n","        for key in label_dict:\n","            p_string = \" \".join([str(start)+\"-\"+str(end) for start,end in label_dict[key]])\n","            writer.writerow({'Id': key, 'Predicted': p_string})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Baselines**\n","\n","On Kaggle, we provide two baselines for you to evaluate your models agaist: **`HMM Baseline`** and **`MEMM Baseline`**. You may use them to internally check your models. In addition, you may get points if for the final submission your best-performing model does better than the **`MEMM baseline`**.\n","\n"],"metadata":{"id":"U1lNil41VqMn"}},{"cell_type":"markdown","metadata":{"id":"BEZP_FGivVRI"},"source":["---\n","<a name=\"q5\"></a>\n","## **Q5: Competition Score**\n","[[^^^]](#outline) \n","\n","\n","Include your team's **best score** and the **name under which that best score was submitted** from Kaggle. "]},{"cell_type":"markdown","source":["#### **A5:**\n","\n","... add your answers here"],"metadata":{"id":"bYirLL5x1mE7"}}]}